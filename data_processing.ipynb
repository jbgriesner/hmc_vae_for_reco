{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import fileinput\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils import concatenate_files, train_tune_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the constants of this notebook are defined in this cell\n",
    "\n",
    "foursquare_raw_DIR = './data/raw/foursquare'\n",
    "gowalla_raw_DIR = './data/raw/gowalla'\n",
    "ml100_raw_DIR = './data/raw/ml-100k'\n",
    "ml25_raw_DIR = './data/raw/ml-25m'\n",
    "\n",
    "foursquare_clean_DIR = './data/clean/foursquare'\n",
    "gowalla_clean_DIR = './data/clean/gowalla'\n",
    "ml100_clean_DIR = './data/clean/ml-100k'\n",
    "ml25_clean_DIR = './data/clean/ml-25m'\n",
    "\n",
    "foursquare_checkins_file = f\"{foursquare_raw_DIR}/checkins\"\n",
    "foursquare_pois_file = f\"{foursquare_raw_DIR}/pois\"\n",
    "gowalla_checkins_file = f\"{gowalla_raw_DIR}/checkins\"\n",
    "gowalla_pois_file = f\"{gowalla_raw_DIR}/pois\"\n",
    "ml100_ratings_file = f\"{ml100_raw_DIR}/ratings.csv\"\n",
    "ml25_ratings_file = f\"{ml25_raw_DIR}/ratings\"\n",
    "\n",
    "foursquare_clicks = 1000000\n",
    "gowalla_clicks = 1000000\n",
    "ml_25_clicks = 1000000\n",
    "\n",
    "# France bounding box\n",
    "lon_min, lat_min, lon_max, lat_max = -5, 40, 10, 52\n",
    "\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating input files\n",
    "- because of file size limitation of github the datasets are splitted in small pieces.\n",
    "- they must be first concatenating into one file\n",
    "- so just do this once to get one file for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_files(foursquare_checkins_file, f\"{foursquare_raw_DIR}/*checkins_0*\")\n",
    "concatenate_files(foursquare_pois_file, f\"{foursquare_raw_DIR}/*pois_0*\")\n",
    "\n",
    "concatenate_files(gowalla_checkins_file, f\"{gowalla_raw_DIR}/*checkins_0*\")\n",
    "concatenate_files(gowalla_pois_file, f\"{gowalla_raw_DIR}/*pois_0*\")\n",
    "\n",
    "concatenate_files(ml25_ratings_file, f\"{ml25_raw_DIR}/*ratings_0*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "- for each of the 4 datasets loaded, the output of the processing are tab separated values (`.tsv`) files in each corresponding `./data/clean/<dataset>` folder\n",
    "- the `.tsv` files are:\n",
    "    * `train.tsv` for training\n",
    "    * `tune.tsv` for validation\n",
    "    * `test.tsv` for test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foursquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset into pandas dataframes\n",
    "# We load only the 2 dimensions corresponding to 'users' and 'items'\n",
    "foursquare_checkins = pd.read_csv(foursquare_checkins_file, error_bad_lines=False, nrows=foursquare_clicks, sep='\\t', usecols=[0,1], names=['user', 'item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get latitude and longitude we also load the 'items' features (including latitudes, longitudes)\n",
    "foursquare_pois = pd.read_csv(foursquare_pois_file, error_bad_lines=False, sep='\\t', usecols=[0,1, 2], names=['item', 'lat', 'lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because we are interested only in the existence of an interaction, we keep only unique pairs \n",
    "foursquare_checkins = foursquare_checkins.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we merge the checkins df with the pois features df\n",
    "foursquare_checkins = pd.merge(left=foursquare_checkins, right=foursquare_pois, left_on='item', right_on='item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total interactions count in Foursquare: 751716\n"
     ]
    }
   ],
   "source": [
    "interactions = foursquare_checkins.shape[0]\n",
    "print(f\"Total interactions count in Foursquare: {interactions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In France we have:\n",
      "\t- distinct users count: 5801\n",
      "\t- distinct items count: 16726\n"
     ]
    }
   ],
   "source": [
    "# we select checkins in France only in order to reduce the dataframe size\n",
    "france_foursquare_checkins = foursquare_checkins[(foursquare_checkins['lon']>lon_min) & \n",
    "           (foursquare_checkins['lon'] < lon_max) & \n",
    "           (foursquare_checkins['lat'] > lat_min) & \n",
    "           (foursquare_checkins['lat'] < lat_max)]\n",
    "\n",
    "users = france_foursquare_checkins['user'].nunique()\n",
    "items = france_foursquare_checkins['item'].nunique()\n",
    "\n",
    "print(f\"In France we have:\\n\\t- distinct users count: {users}\\n\\t- distinct items count: {items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- users: 5801\n",
      "- items: 16726\n",
      "- density = 0.02%\n"
     ]
    }
   ],
   "source": [
    "# we convert the dataframe into a scipy sparse matrix\n",
    "pivotable = france_foursquare_checkins.groupby(['user', 'item'], sort=False).size().unstack(fill_value=0)\n",
    "X = scipy.sparse.csr_matrix(pivotable)\n",
    "users, items = X.shape\n",
    "\n",
    "print(f\"- users: {users}\\n- items: {items}\")\n",
    "print(f\"- density = {float(round((X.getnnz() / np.prod(X.shape))*100,2))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5801it [00:01, 5481.63it/s]\n"
     ]
    }
   ],
   "source": [
    "#Finally create the train, tune, test files\n",
    "train_tune_test_split(foursquare_clean_DIR, X, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gowalla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset into pandas dataframes\n",
    "# We load only the 2 dimensions corresponding to 'users' and 'items'\n",
    "gowalla_checkins = pd.read_csv(gowalla_checkins_file, error_bad_lines=False, nrows=gowalla_clicks, usecols=[0,1], names=['user', 'item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jb/.cache/pypoetry/virtualenvs/hmc-vae-for-reco-vJuBCjp2-py3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# To get latitude and longitude we also load the 'items' features (including latitudes, longitudes)\n",
    "gowalla_pois = pd.read_csv(gowalla_pois_file, error_bad_lines=False, usecols=[0,2,3], names=['item', 'lon', 'lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we did above, we keep only unique pairs \n",
    "gowalla_checkins = gowalla_checkins.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we merge the checkins df with the pois features df\n",
    "gowalla_checkins = pd.merge(left=gowalla_checkins, right=gowalla_pois, left_on='item', right_on='item')\n",
    "gowalla_checkins['lon'] = pd.to_numeric(gowalla_checkins['lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total interactions count: 545233\n"
     ]
    }
   ],
   "source": [
    "interactions = gowalla_checkins.shape[0]\n",
    "print(f\"Total interactions count: {interactions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we select checkins in France only in order to reduce the dataframe size\n",
    "france_gowalla_checkins = gowalla_checkins[(gowalla_checkins['lon']>lon_min) & \n",
    "           (gowalla_checkins['lon'] < lon_max) & \n",
    "           (gowalla_checkins['lat'] > lat_min) & \n",
    "           (gowalla_checkins['lat'] < lat_max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- users: 1167\n",
      "- items: 24504\n",
      "- density = 0.12%\n"
     ]
    }
   ],
   "source": [
    "# we convert the dataframe into a scipy sparse matrix\n",
    "pivotable = france_gowalla_checkins.groupby(['user', 'item'], sort=False).size().unstack(fill_value=0)\n",
    "X = scipy.sparse.csr_matrix(pivotable)\n",
    "users, items = X.shape\n",
    "print(f\"- users: {users}\\n- items: {items}\")\n",
    "print(f\"- density = {float(round((X.getnnz() / np.prod(X.shape))*100,2))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1167it [00:00, 4831.28it/s]\n"
     ]
    }
   ],
   "source": [
    "#Finally create the train, tune, test files\n",
    "train_tune_test_split(gowalla_clean_DIR, X, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ml-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset into pandas dataframes\n",
    "ml100_ratings = pd.read_csv(ml100_ratings_file, error_bad_lines=False, header=0, usecols=[0, 1, 2], names=['user', 'item', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize the data (only keep ratings >= 4)\n",
    "ml100_ratings = ml100_ratings[ml100_ratings['rating'] > 3.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total interactions count: 48580\n"
     ]
    }
   ],
   "source": [
    "# as we did above, we keep only unique pairs \n",
    "ml100_ratings = ml100_ratings.drop_duplicates()\n",
    "interactions = ml100_ratings.shape[0]\n",
    "print(f\"Total interactions count: {interactions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- users: 609\n",
      "- items: 6298\n",
      "- density = 1.27%\n"
     ]
    }
   ],
   "source": [
    "# we convert the dataframe into a scipy sparse matrix\n",
    "pivotable = ml100_ratings.groupby(['user', 'item'], sort=False).size().unstack(fill_value=0)\n",
    "X = scipy.sparse.csr_matrix(pivotable)\n",
    "users, items = X.shape\n",
    "print(f\"- users: {users}\\n- items: {items}\")\n",
    "print(f\"- density = {float(round((X.getnnz() / np.prod(X.shape))*100,2))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "609it [00:00, 4093.49it/s]\n"
     ]
    }
   ],
   "source": [
    "#Finally create the train, tune, test files\n",
    "train_tune_test_split(ml100_clean_DIR, X, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ml-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset into pandas dataframes\n",
    "ml25_ratings = pd.read_csv(ml25_ratings_file, error_bad_lines=False, header=0, nrows=ml_25_clicks, usecols=[0, 1, 2], names=['user', 'item', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize the data (only keep ratings >= 4)\n",
    "ml25_ratings = ml25_ratings[ml25_ratings['rating'] > 3.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total interactions count: 499262\n"
     ]
    }
   ],
   "source": [
    "# as we did above, we keep only unique pairs \n",
    "ml25_ratings = ml25_ratings.drop_duplicates()\n",
    "interactions = ml25_ratings.shape[0]\n",
    "print(f\"Total interactions count: {interactions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- users: 6370\n",
      "- items: 15814\n",
      "- density = 0.5%\n"
     ]
    }
   ],
   "source": [
    "# we convert the dataframe into a scipy sparse matrix\n",
    "pivotable = ml25_ratings.groupby(['user', 'item'], sort=False).size().unstack(fill_value=0)\n",
    "X = scipy.sparse.csr_matrix(pivotable)\n",
    "users, items = X.shape\n",
    "print(f\"- users: {users}\\n- items: {items}\")\n",
    "print(f\"- density = {float(round((X.getnnz() / np.prod(X.shape))*100,2))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6370it [00:01, 4287.84it/s]\n"
     ]
    }
   ],
   "source": [
    "#Finally create the train, tune, test files\n",
    "train_tune_test_split(ml25_clean_DIR, X, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
