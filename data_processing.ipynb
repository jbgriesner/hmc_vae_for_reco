{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import fileinput\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the constants of this notebook are defined in this cell\n",
    "\n",
    "foursquare_raw_DIR = './data/raw/foursquare'\n",
    "gowalla_raw_DIR = './data/raw/gowalla'\n",
    "ml100_raw_DIR = './data/raw/ml-100k'\n",
    "ml25_raw_DIR = './data/raw/ml-25m'\n",
    "\n",
    "foursquare_clean_DIR = './data/clean/foursquare'\n",
    "gowalla_clean_DIR = './data/clean/gowalla'\n",
    "ml100_clean_DIR = './data/clean/ml-100k'\n",
    "ml25_clean_DIR = './data/clean/ml-25m'\n",
    "\n",
    "foursquare_checkins_file = f\"{foursquare_raw_DIR}/checkins\"\n",
    "foursquare_pois_file = f\"{foursquare_raw_DIR}/pois\"\n",
    "gowalla_checkins_file = f\"{gowalla_raw_DIR}/checkins\"\n",
    "gowalla_pois_file = f\"{gowalla_raw_DIR}/pois\"\n",
    "ml100_ratings_file = f\"{ml100_raw_DIR}/ratings.csv\"\n",
    "ml25_ratings_file = f\"{ml25_raw_DIR}/ratings\"\n",
    "\n",
    "foursquare_clicks = 1000000\n",
    "gowalla_clicks = 1000000\n",
    "ml_25_clicks = 1000000\n",
    "\n",
    "# France bounding box\n",
    "lon_min, lat_min, lon_max, lat_max = -5, 40, 10, 52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating input files\n",
    "- because of file size limitation of github the datasets are splitted in small pieces.\n",
    "- they must be first concatenating into one file\n",
    "- so just do this once to get one file for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_files(file, pattern):\n",
    "    \"\"\"\n",
    "        If the ratings/checkins file exists for each dataset, we do nothing,\n",
    "        otherwise we just concatenate the pieces of files\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file):\n",
    "        checkins_files = glob.glob(pattern)\n",
    "        with open(file, 'w') as out_file:\n",
    "            input_lines = fileinput.input(checkins_files)\n",
    "            prev_line = None\n",
    "            for line in tqdm(input_lines):\n",
    "                if not fileinput.isfirstline():  # first lines are corrupted\n",
    "                    if prev_line is not None:    # last lines are corrupted\n",
    "                        out_file.write(prev_line)\n",
    "                    prev_line = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_files(foursquare_checkins_file, f\"{foursquare_raw_DIR}/*checkins_0*\")\n",
    "concatenate_files(foursquare_pois_file, f\"{foursquare_raw_DIR}/*pois_0*\")\n",
    "\n",
    "concatenate_files(gowalla_checkins_file, f\"{gowalla_raw_DIR}/*checkins_0*\")\n",
    "concatenate_files(gowalla_pois_file, f\"{gowalla_raw_DIR}/*pois_0*\")\n",
    "\n",
    "concatenate_files(ml25_ratings_file, f\"{ml25_raw_DIR}/*ratings_0*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "- for each of the 4 datasets loaded, the output of the processing are tab separated values (`.tsv`) files in each corresponding `./data/clean/<dataset>` folder\n",
    "- the `.tsv` files are:\n",
    "    * `train.tsv` for training\n",
    "    * `tune.tsv` for validation\n",
    "    * `test.tsv` for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tune_test_split(clean_DIR, X, test_min_clicks):\n",
    "    \"\"\"\n",
    "        Split the 'X' matrix into train, tune and test .tsv files\n",
    "        in the given 'clean_DIR' folder.        \n",
    "    \"\"\"\n",
    "    train_file = f\"{clean_DIR}/train.tsv\"\n",
    "    tune_file = f\"{clean_DIR}/tune.tsv\"\n",
    "    test_file = f\"{clean_DIR}/test.tsv\"\n",
    "    \n",
    "    if os.path.exists(train_file):\n",
    "        os.remove(train_file)\n",
    "    if os.path.exists(tune_file):\n",
    "        os.remove(tune_file)\n",
    "    if os.path.exists(test_file):\n",
    "        os.remove(test_file)\n",
    "\n",
    "    with open(train_file, 'w') as train, open(tune_file, 'w') as tune, open(test_file, 'w') as test:\n",
    "        user = 0\n",
    "        for row in tqdm(X): \n",
    "            items = row.nonzero()[1]\n",
    "            np.random.shuffle(items)\n",
    "            if row.sum() > test_min_clicks:                       \n",
    "                splits = np.split(items, [int(.8 * len(items)), int(.9 * len(items))])  \n",
    "                for item in splits[0]:\n",
    "                        train.write(f\"{user}\\t{item}\\n\")\n",
    "                for item in splits[1]:\n",
    "                        tune.write(f\"{user}\\t{item}\\n\")\n",
    "                for item in splits[2]:\n",
    "                        test.write(f\"{user}\\t{item}\\n\")\n",
    "            else:\n",
    "                for item in items:\n",
    "                    train.write(f\"{user}\\t{item}\\n\")\n",
    "            user += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foursquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset into pandas dataframes\n",
    "# We load only the 2 dimensions corresponding to 'users' and 'items'\n",
    "foursquare_checkins = pd.read_csv(foursquare_checkins_file, error_bad_lines=False, nrows=foursquare_clicks, sep='\\t', usecols=[0,1], names=['user', 'item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get latitude and longitude we also load the 'items' features (including latitudes, longitudes)\n",
    "foursquare_pois = pd.read_csv(foursquare_pois_file, error_bad_lines=False, sep='\\t', usecols=[0,1, 2], names=['item', 'lat', 'lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because we are interested only in the existence of an interaction, we keep only unique pairs \n",
    "foursquare_checkins = foursquare_checkins.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we merge the checkins df with the pois features df\n",
    "foursquare_checkins = pd.merge(left=foursquare_checkins, right=foursquare_pois, left_on='item', right_on='item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total interactions count in Foursquare: 751716\n"
     ]
    }
   ],
   "source": [
    "interactions = foursquare_checkins.shape[0]\n",
    "print(f\"Total interactions count in Foursquare: {interactions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In France we have:\n",
      "\t- distinct users count: 5801\n",
      "\t- distinct items count: 16726\n"
     ]
    }
   ],
   "source": [
    "# we select checkins in France only in order to reduce the dataframe size\n",
    "france_foursquare_checkins = foursquare_checkins[(foursquare_checkins['lon']>lon_min) & \n",
    "           (foursquare_checkins['lon'] < lon_max) & \n",
    "           (foursquare_checkins['lat'] > lat_min) & \n",
    "           (foursquare_checkins['lat'] < lat_max)]\n",
    "\n",
    "users = france_foursquare_checkins['user'].nunique()\n",
    "items = france_foursquare_checkins['item'].nunique()\n",
    "\n",
    "print(f\"In France we have:\\n\\t- distinct users count: {users}\\n\\t- distinct items count: {items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- users: 5801\n",
      "- items: 16726\n",
      "- density = 0.02%\n"
     ]
    }
   ],
   "source": [
    "# we convert the dataframe into a scipy sparse matrix\n",
    "pivotable = france_foursquare_checkins.groupby(['user', 'item'], sort=False).size().unstack(fill_value=0)\n",
    "X = scipy.sparse.csr_matrix(pivotable)\n",
    "users, items = X.shape\n",
    "\n",
    "print(f\"- users: {users}\\n- items: {items}\")\n",
    "print(f\"- density = {float(round((X.getnnz() / np.prod(X.shape))*100,2))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5801it [00:01, 5476.65it/s]\n"
     ]
    }
   ],
   "source": [
    "#Finally create the train, tune, test files\n",
    "train_tune_test_split(foursquare_clean_DIR, X, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gowalla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset into pandas dataframes\n",
    "# We load only the 2 dimensions corresponding to 'users' and 'items'\n",
    "gowalla_checkins = pd.read_csv(gowalla_checkins_file, error_bad_lines=False, nrows=gowalla_clicks, usecols=[0,1], names=['user', 'item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get latitude and longitude we also load the 'items' features (including latitudes, longitudes)\n",
    "gowalla_pois = pd.read_csv(gowalla_pois_file, error_bad_lines=False, usecols=[0,2,3], names=['item', 'lon', 'lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we did above, we keep only unique pairs \n",
    "gowalla_checkins = gowalla_checkins.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we merge the checkins df with the pois features df\n",
    "gowalla_checkins = pd.merge(left=gowalla_checkins, right=gowalla_pois, left_on='item', right_on='item')\n",
    "gowalla_checkins['lon'] = pd.to_numeric(gowalla_checkins['lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total interactions count: 545233\n"
     ]
    }
   ],
   "source": [
    "interactions = gowalla_checkins.shape[0]\n",
    "print(f\"Total interactions count: {interactions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we select checkins in France only in order to reduce the dataframe size\n",
    "france_gowalla_checkins = gowalla_checkins[(gowalla_checkins['lon']>lon_min) & \n",
    "           (gowalla_checkins['lon'] < lon_max) & \n",
    "           (gowalla_checkins['lat'] > lat_min) & \n",
    "           (gowalla_checkins['lat'] < lat_max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- users: 1167\n",
      "- items: 24504\n",
      "- density = 0.12%\n"
     ]
    }
   ],
   "source": [
    "# we convert the dataframe into a scipy sparse matrix\n",
    "pivotable = france_gowalla_checkins.groupby(['user', 'item'], sort=False).size().unstack(fill_value=0)\n",
    "X = scipy.sparse.csr_matrix(pivotable)\n",
    "users, items = X.shape\n",
    "print(f\"- users: {users}\\n- items: {items}\")\n",
    "print(f\"- density = {float(round((X.getnnz() / np.prod(X.shape))*100,2))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1167it [00:00, 4839.81it/s]\n"
     ]
    }
   ],
   "source": [
    "#Finally create the train, tune, test files\n",
    "train_tune_test_split(gowalla_clean_DIR, X, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ml-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset into pandas dataframes\n",
    "ml100_ratings = pd.read_csv(ml100_ratings_file, error_bad_lines=False, header=0, usecols=[0, 1, 2], names=['user', 'item', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize the data (only keep ratings >= 4)\n",
    "ml100_ratings = ml100_ratings[ml100_ratings['rating'] > 3.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total interactions count: 48580\n"
     ]
    }
   ],
   "source": [
    "# as we did above, we keep only unique pairs \n",
    "ml100_ratings = ml100_ratings.drop_duplicates()\n",
    "interactions = ml100_ratings.shape[0]\n",
    "print(f\"Total interactions count: {interactions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- users: 609\n",
      "- items: 6298\n",
      "- density = 1.27%\n"
     ]
    }
   ],
   "source": [
    "# we convert the dataframe into a scipy sparse matrix\n",
    "pivotable = ml100_ratings.groupby(['user', 'item'], sort=False).size().unstack(fill_value=0)\n",
    "X = scipy.sparse.csr_matrix(pivotable)\n",
    "users, items = X.shape\n",
    "print(f\"- users: {users}\\n- items: {items}\")\n",
    "print(f\"- density = {float(round((X.getnnz() / np.prod(X.shape))*100,2))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "609it [00:00, 4133.13it/s]\n"
     ]
    }
   ],
   "source": [
    "#Finally create the train, tune, test files\n",
    "train_tune_test_split(ml100_clean_DIR, X, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ml-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset into pandas dataframes\n",
    "ml25_ratings = pd.read_csv(ml25_ratings_file, error_bad_lines=False, header=0, nrows=ml_25_clicks, usecols=[0, 1, 2], names=['user', 'item', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize the data (only keep ratings >= 4)\n",
    "ml25_ratings = ml25_ratings[ml25_ratings['rating'] > 3.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total interactions count: 499262\n"
     ]
    }
   ],
   "source": [
    "# as we did above, we keep only unique pairs \n",
    "ml25_ratings = ml25_ratings.drop_duplicates()\n",
    "interactions = ml25_ratings.shape[0]\n",
    "print(f\"Total interactions count: {interactions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- users: 6370\n",
      "- items: 15814\n",
      "- density = 0.5%\n"
     ]
    }
   ],
   "source": [
    "# we convert the dataframe into a scipy sparse matrix\n",
    "pivotable = ml25_ratings.groupby(['user', 'item'], sort=False).size().unstack(fill_value=0)\n",
    "X = scipy.sparse.csr_matrix(pivotable)\n",
    "users, items = X.shape\n",
    "print(f\"- users: {users}\\n- items: {items}\")\n",
    "print(f\"- density = {float(round((X.getnnz() / np.prod(X.shape))*100,2))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6370it [00:01, 4230.25it/s]\n"
     ]
    }
   ],
   "source": [
    "#Finally create the train, tune, test files\n",
    "train_tune_test_split(ml25_clean_DIR, X, 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
